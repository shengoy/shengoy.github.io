<!DOCTYPE html><html><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><title>One Transformer Can Understand Both 2D &amp; 3D Molecular Data | magmell</title><meta name=author content=magmell><meta name=copyright content=magmell><link rel="shortcut icon" href=http://magmell.site/img/My.jpg><meta name=keywords content=笔记><meta name=description content="网站对latex的支持一言难尽，所以以后不会再更新论文笔记了  论文|代码  architecture：主干网络仍然是Transformer，上半部分处理2D数据：度、边以及最短路径；下半部分处理3D数据：结点间的距离信息。将与结点有关的信息加到结点的特征里去，起到一个特征增强的作用。将与结点对&#x2F;边有关的信息当作一个偏置项加到attention上，起到一个相对位置编码的作用。 本文只在"><meta property=og:type content=article><meta property=og:title content="One Transformer Can Understand Both 2D &amp; 3D Molecular Data"><meta property=og:url content=http://magmell.site/onetransformer/ ><meta property=og:site_name content=magmell><meta property=og:description content="网站对latex的支持一言难尽，所以以后不会再更新论文笔记了  论文|代码  architecture：主干网络仍然是Transformer，上半部分处理2D数据：度、边以及最短路径；下半部分处理3D数据：结点间的距离信息。将与结点有关的信息加到结点的特征里去，起到一个特征增强的作用。将与结点对&#x2F;边有关的信息当作一个偏置项加到attention上，起到一个相对位置编码的作用。 本文只在"><meta property=og:locale><meta property=og:image content=http://magmell.site/img/My.jpg><meta property=article:published_time content=2022-10-21T16:02:26.000Z><meta property=article:modified_time content=2022-10-21T16:32:20.000Z><meta property=article:author content=magmell><meta property=article:tag content=笔记><meta name=twitter:card content=summary><meta name=twitter:image content=http://magmell.site/img/My.jpg><meta http-equiv=Cache-Control content=no-siteapp><link href=//cdn.jsdelivr.net rel=preconnect><link href=http://magmell.site rel=prefetch><link href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet><script src=/js/utlis.js>"use strict";</script><meta name=generator content="Hexo 5.4.1"></head><body><script>"use strict";var $config={tocStyle:"visible",CDN:{fancyboxJs:"https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js",fancyboxCss:"https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css"},searchFile:"/search.xml",codeBlockExpand:{enable:!0,height:400,scrollTop:200}};</script><script>"use strict";var _hmt=_hmt||[];!function(){var t=document.createElement("script"),e=(t.src="https://hm.baidu.com/hm.js?b8312b70c827f3279376517c136f52f5",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}();</script><script>"use strict";var script=document.createElement("script");function gtag(){dataLayer.push(arguments)}script.src="https://www.googletagmanager.com/gtag/js?id=G-DHWF0JVX7P",script.async=!0,document.head.appendChild(script),window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DHWF0JVX7P");</script><script>"use strict";document.addEventListener("pjax:complete",function(){"undefined"!=typeof _hmt&&"function"==typeof _hmt.push&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof ga&&ga("send","pageview",window.location.href),"function"==typeof gtag&&gtag("event","pageview",{page_location:window.location.href})});</script><div id=body-wrap><nav id=nav-wrap><div class=navbar><div class=bar><a href=/ class=title>magmell</a> <i class="fas fa-search search-btn"></i><ul class=menu><li><a href=/ >首页</a></li><li><a class=menu-child-hover href=javascript:void(0);>找文章</a><ul class=menu-child><li><a href=/tags>标签</a></li><li><a href=/categories>分类</a></li><li><a href=/archives>归档</a></li></ul></li><li><a href=/about>关于我</a></li></ul><i class="fas fa-bars open-nav"></i></div></div><div id=mobile-nav><ul><li><a href=/ >首页</a></li><li><a href=/tags>标签</a></li><li><a href=/categories>分类</a></li><li><a href=/archives>归档</a></li><li><a href=/about>关于我</a></li></ul></div></nav><main id=main><article id=post><div class=post-info><div class=post-title><h1>One Transformer Can Understand Both 2D &amp; 3D Molecular Data</h1></div><div class=post-meta><div class=post-date><i class="far fa-calendar-alt fa-fw post-meta-icon"></i> <span class=post-meta-label>发表于 2022-10-22 | </span><i class="fas fa-history fa-fw post-meta-icon"></i> <span class=post-meta-label>更新于 2023-08-21</span></div><div class=post-meta-wordcount><i class="far fa-file-word fa-fw post-meta-icon"></i> <span class=post-meta-label>总字数:</span> <span class=word-count>1.6k | </span><i class="far fa-clock fa-fw post-meta-icon"></i> <span class=post-meta-label>阅读时长:</span> <span>8分钟</span> | <i class="far fa-eye fa-fw post-meta-icon"></i> <span id=/onetransformer/ class=leancloud_visitors data-flag-title="One Transformer Can Understand Both 2D &amp; 3D Molecular Data"><span class=post-meta-label>阅读量:</span> <span class=leancloud-visitors-count>0</span></span></div></div></div><div class=post-content><blockquote><p>网站对latex的支持一言难尽，所以以后不会再更新论文笔记了</p></blockquote><p><a target=_blank rel=noopener href=https://arxiv.org/abs/2210.01765>论文</a>|<a target=_blank rel=noopener href=https://github.com/lsj2408/Transformer-M>代码</a></p><p><img src=http://tva1.sinaimg.cn/large/006XaTquly1h7dcu8p4hcj31aa0qlthu.jpg alt=image-20221020212212253></p><p>architecture：主干网络仍然是Transformer，上半部分处理2D数据：度、边以及最短路径；下半部分处理3D数据：结点间的距离信息。将与结点有关的信息加到结点的特征里去，起到一个特征增强的作用。将与结点对&#x2F;边有关的信息当作一个偏置项加到attention上，起到一个相对位置编码的作用。</p><p>本文只在<a target=_blank rel=noopener href=https://proceedings.neurips.cc/paper/2021/hash/f1c1592588411002af340cbaedd6fc33-Abstract.html>Ying C, Cai T, Luo S, et al. Do transformers really perform badly for graph representation?[J]. Advances in Neural Information Processing Systems, 2021, 34: 28877-28888.</a> 的基础上加了3D数据的信息（NIPS21 这篇论文发表时3D数据还没放出来），<del>鉴定为水</del>。</p><hr><h2 id=Background><a href=#Background class=headerlink title=Background></a>Background</h2><p>分子数据可以由不同的形式表示，比如可以用类似化学分子式这样的字符串表示，可以把原子看作结点、化学键看作边表示成二维图（graph）的形式，也可以用各原子在三维空间中的位置表示。所以本文想用一个模型同时输入2D和3D数据，得到有意义的表征。</p><blockquote><p>molecules can naturally be characterized using different chemical formulations.</p><p>molecules naturally have different chemical formulations.</p><p>in this work, we develop a novel Transformer-based Molecular model called Transformer-M, which can take molecular data of 2D or 3D formats as input and generate meaningful semantic representations.</p></blockquote><h2 id=TRANSFORMER-M><a href=#TRANSFORMER-M class=headerlink title=TRANSFORMER-M></a>TRANSFORMER-M</h2><p>输入：3D数据提供了每个原子的笛卡尔坐标系（三维）</p><p>两个通道，分别处理2D数据和3D数据。2D通道将最短路径encoding和边encoding加入attention的偏置，将度encoding加入结点特征；3D通道将结点间的欧式距离的encoding加入偏置，将结点与其它结点的距离之和的encoding加入特征。</p><p>位置编码对非序列数据很重要</p><blockquote><p>Shortly, many works realized that positional encoding plays a crucial role in extending standard Transformer to more complicated data structures beyond language.</p></blockquote><h3 id=Transformer-layer><a href=#Transformer-layer class=headerlink title="Transformer layer"></a>Transformer layer</h3><p>$$<br>\begin{equation}<br>\boldsymbol{A}^h\left(\boldsymbol{X}^{(l)}\right)&#x3D;\operatorname{softmax}\left(\frac{\boldsymbol{X}^{(l)} \boldsymbol{W}_Q^{l, h}\left(\boldsymbol{X}^{(l)} \boldsymbol{W}_K^{l, h}\right)^{\top}}{\sqrt{d}}\right)<br>\end{equation}<br>$$</p><p>$$<br>\begin{gathered}<br>\hat{\boldsymbol{X}}^{(l)}&#x3D;\boldsymbol{X}^{(l)}+\sum_{h&#x3D;1}^H \boldsymbol{A}^h\left(\boldsymbol{X}^{(l)}\right) \boldsymbol{X}^{(l)} \boldsymbol{W}_V^{l, h} \boldsymbol{W}_O^{l, h} ; \<br>\boldsymbol{X}^{(l+1)}&#x3D;\hat{\boldsymbol{X}}^{(l)}+\operatorname{GELU}\left(\hat{\boldsymbol{X}}^{(l)} \boldsymbol{W}_1^l\right) \boldsymbol{W}_2^l<br>\end{gathered}<br>$$</p><h3 id=Encoding-pair-wise-relations-in-E><a href=#Encoding-pair-wise-relations-in-E class=headerlink title="Encoding pair-wise relations in E"></a>Encoding pair-wise relations in E</h3><p>最短路径和边的encoding都是一个n*n的矩阵,里面每一个值都表示两个结点之间的关系。</p><blockquote><p>Denote $\Phi^{\text{SPD}}$ and $\Phi^{\text{Edge}}$ as the matrix form of the SPD encoding and edge encoding, both of which are of shape $n\times n$.</p></blockquote><p>最短路径</p><p>$$\text{SP}_{ij}&#x3D;(\vec{e}_1,\vec{e}_2,…,\vec{e}_N)$$</p><p>边encoding</p>$$\Phi^{\text{Edge}}_{ij}=\frac{1}{N}\sum_{n=1}^{N} \vec{e}_n(w_{n})^T$$<h3 id=Encoding-pair-wise-relations-in-R><a href=#Encoding-pair-wise-relations-in-R class=headerlink title="Encoding pair-wise relations in R"></a>Encoding pair-wise relations in R</h3>$$\Phi^{\text{3D Distance}}$$ 也是一个n\*n的矩阵，里面每个值都反映了两个结点间的空间关系。 $$ {\Phi^{\text{3D Distance}}_{ij}}=\operatorname{GELU}\left(\boldsymbol{\psi}_{(i, j)} \boldsymbol{W}_D^1\right) \boldsymbol{W}_D^2 $$<p>其中</p>$$ \boldsymbol{\psi}_{(i, j)}=\left[\psi_{(i, j)}^1 ; \ldots ; \psi_{(i, j)}^K\right]^{\top}, \boldsymbol{W}_D^1 \in \mathbb{R}^{K \times K}, \boldsymbol{W}_D^2 \in \mathbb{R}^{K \times 1} $$<p>K个高斯核函数</p>$$ {\psi}^{k}_{{i,j}}=-\frac{1}{\sqrt{2 \pi}\left|\sigma^k\right|} \exp \left(-\frac{1}{2}\left(\frac{\gamma_{(i, j)}\left\|\mid \mathbf{r}_i-\mathbf{r}_j\right\|+\beta_{(i, j)}-\mu^k}{\left|\sigma^k\right|}\right)^2\right), k=1, \ldots, K $$<p>将上面三个encoding当作偏置项加入attention中</p>$$ \boldsymbol{A}(\boldsymbol{X})=\operatorname{softmax}(\frac{\boldsymbol{X} \boldsymbol{W}_Q\left(\boldsymbol{X} \boldsymbol{W}_K\right)^{\top}}{\sqrt{d}}+\underbrace{\Phi^{\mathrm{SPD}}+\Phi^{\text {Edge }}}_{2 \mathrm{D} \text { pair-wise channel }}+\underbrace{\Phi^{3 \mathrm{D} \text { Distance }}}_{\text {3D pair-wise channel }}) $$<h3 id=Encoding-atom-wise-structural-information-in-E><a href=#Encoding-atom-wise-structural-information-in-E class=headerlink title="Encoding atom-wise structural information in E"></a>Encoding atom-wise structural information in E</h3><p>度的encoding，是一个n*d的矩阵，一般会有一个可学习的embedding，根据每个结点的度去得到一个向量，而且一般会设置一个阈值，度数大于某个值的向量就一样了。</p>$$ \Psi^{\text{Degree}}=[\Psi^{\text{Degree}}_{1},\Psi^{\text{Degree}}_{2},...,\Psi^{\text{Degree}}_{n}] $$<h3 id=Encoding-atom-wise-structural-information-in-R><a href=#Encoding-atom-wise-structural-information-in-R class=headerlink title="Encoding atom-wise structural information in R"></a>Encoding atom-wise structural information in R</h3><p>距离和的encoding，是一个n*d的矩阵</p>$$ \Psi^{\text{Sum of 3D Distance}}_{i}=\sum_{j\in [n]}{{\psi}_{{i,j}}}\boldsymbol{W}_D^3 $$ $$ \boldsymbol{X}^{(0)}=\boldsymbol{X}+\underbrace{\Psi^{\text {Degree }}}_{2 \mathrm{D} \text { atom-wise channel }}+\underbrace{\Psi^{\text {Sum of 3D Distance }}}_{3 \mathrm{D} \text { atom-wise channel }} $$<p>2D数据和3D数据具有一定互补性。比如2D数据只包含了化学键的类型，3D数据包含了细粒度的化学键的长度以及角度。</p><blockquote><p>For example, the 2D graph structure only contains bonds with bond type, while the 3D geometric structure contains fine-grained information such as lengths and angles. As another example, the 3D geometric structures are usually obtained from computational simulations like Density Functional Theory (DFT) (Burke, 2012), which could have approximation errors. The 2D graphs are constructed by domain experts, which to some extent, provide references to the 3D structure.</p></blockquote><h2 id=EXPERIMENTS><a href=#EXPERIMENTS class=headerlink title=EXPERIMENTS></a>EXPERIMENTS</h2><p>训练的时候根据一个预定义的分布随机采用一种数据类型（2D、3D、2D+3D），类似dropout机制。</p><blockquote><p>we provide three modes for each data instance: (1) activate the 2D channels and disable the 3D channels (2D mode); (2) activate the 3D channels and disable the 2D channels (3D mode); (3) activate both channels (2D+3D mode). The mode of each data instance during training is randomly drawn on the fly according to a pre-defined distribution</p></blockquote><p>训练的时候除了监督学习的目标函数还使用了一种自监督的目标函数</p><blockquote><p>Besides, we also use a self-supervised learning objective called 3D Position Denoising</p><p>During training, if a data instance is in the 3D mode, we add Gaussian noise to the position of each atom and require the model to predict the noise from the noisy input.</p></blockquote><h3 id=PCQM4MV2-PERFORMANCE-2D><a href=#PCQM4MV2-PERFORMANCE-2D class=headerlink title="PCQM4MV2 PERFORMANCE (2D)"></a>PCQM4MV2 PERFORMANCE (2D)</h3><p>分子属性预测，回归问题</p><p><img src=http://tva3.sinaimg.cn/large/006XaTquly1h7dcumozkfj317x0q7dy6.jpg alt=image-20221020230229281></p><blockquote><p>the 2D-3D joint training with shared parameters indeed helps the model learn more chemical knowledge.</p></blockquote><h3 id=PDBBIND-PERFORMANCE-2D-amp-3D><a href=#PDBBIND-PERFORMANCE-2D-amp-3D class=headerlink title="PDBBIND PERFORMANCE (2D &amp; 3D)"></a>PDBBIND PERFORMANCE (2D &amp; 3D)</h3><p>回归问题</p><p>数据集：</p><blockquote><p>one of the most widely used datasets for structurebased virtual screening</p><p>PDBBind dataset consists of protein-ligand complexes as data instances, which are obtained in bioassay experiments associated with the pKa (or − log Kd, − log Ki) affinity values.</p><p>The task requires models to predict the binding affinity of protein-ligand complexes</p></blockquote><p><img src=http://tvax2.sinaimg.cn/large/006XaTquly1h7dcw4gxcqj31dt0q2kbg.jpg alt=image-20221020230528491></p><blockquote><p>It is worth noting that data instances of the PDBBind dataset are protein-ligand complexes, while our model is pre-trained on simple molecules, demonstrating the transferability of Transformer-M.</p></blockquote><h3 id=QM9-PERFORMANCE-3D><a href=#QM9-PERFORMANCE-3D class=headerlink title="QM9 PERFORMANCE (3D)"></a>QM9 PERFORMANCE (3D)</h3><p>回归问题</p><p>数据集：</p><blockquote><p>QM9 is a quantum chemistry benchmark consisting of 134k stable small organic molecules.</p><p>Each molecule is associated with 12 targets covering its energetic, electronic, and thermodynamic properties.</p></blockquote><p><img src=http://tva2.sinaimg.cn/large/006XaTquly1h7dcwar5t4j31f80srhb1.jpg alt=image-20221020230915827></p><p>In particular, Transformer-M performs best on HUMO, LUMO, and HUMO-LUMO predictions. This indicates that the knowledge learned in the pre-training task transfers better to similar tasks.</p><h3 id=ABLATION-STUDY><a href=#ABLATION-STUDY class=headerlink title="ABLATION STUDY"></a>ABLATION STUDY</h3><h4 id=Impact-of-the-pre-training-tasks><a href=#Impact-of-the-pre-training-tasks class=headerlink title="Impact of the pre-training tasks"></a>Impact of the pre-training tasks</h4><p><img src=http://tvax3.sinaimg.cn/large/006XaTquly1h7dcwfr7dbj30uc0cttb0.jpg alt=image-20221020230952436></p><blockquote><p>It can be seen that the joint pre-training significantly boosts the performance on both datasets. Besides, the 3D Position Denoising task is also beneficial, especially on the QM9 dataset.</p></blockquote><h4 id=Impact-of-mode-distribution><a href=#Impact-of-mode-distribution class=headerlink title="Impact of mode distribution"></a>Impact of mode distribution</h4><p><img src=http://tva2.sinaimg.cn/large/006XaTquly1h7dcwkgj0lj30kh0ae3zo.jpg alt=image-20221020231043710></p><hr></div><div class=post-copyright><div class=post-copyright-icon></div><div class=post-copyright-author><span class=post-copyright-meta>文章作者: </span><span class=post-copyright-info><a href=mailto:shengouy@stu.xmu.edu.cn>shoy</a></span></div><div class=post-copyright-type><span class=post-copyright-meta>文章链接: </span><span class=post-copyright-info><a href=http://magmell.site/onetransformer/ >http://magmell.site/onetransformer/</a></span></div><div class=post-copyright-notice><span class=post-copyright-meta>版权声明: </span><span class=post-copyright-info>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>CC BY-NC-SA 4.0 </a>许可协议。转载请注明来自 <a href=http://magmell.site/onetransformer/ target=_blank>blog from magmell</a> ！</span></div></div><div class=post-tag><span><i class="fa fa-tag"></i> <a href=tags/笔记/ ><span>笔记</span></a></span></div><div class=pagination-post><a href=/hetgraph/ ><div class=prev-title><i class="fas fa-chevron-left"></i>hetgraph</div><div class=prev-desc></div></a><a href=/tenlessions/ ><div class=next-title>Ten Lessons I wish I had been Taught<i class="fas fa-chevron-right"></i></div><div class=next-desc>原文： Ten Lessons I wish I had been Taught 演讲 一个主题 一场演讲应该只有一个主题，其中的主要观点应该被反复强调。 不要超时 One minute o...</div></a></div><div class=comment-head id=直达评论><hr><div class=comment-headline><i class="fas fa-comments fa-fw"></i> <span>评论</span></div><div id=vcomments></div><script>"use strict";function LoadValine(){getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js",function(){var e={el:"#vcomments",path:window.location.pathname,appId:"uYPQylOCyVdXUjwoeA1Qlo9r-gzGzoHsz",appKey:"fhFK17SLElipGtq3bmkD9AGi",master:"",friends:"",placeholder:"快来评论吧!!",avatar:"",meta:"nick,mail,link".split(","),pageSize:5,lang:"zh-CN",emojiCDN:"",emojiMaps:null,enableQQ:!0,visitor:!0};new Valine(e)})}LoadValine();</script></div></article><div id=toc-wrap><div id=toc><div class=toc-title><div>目录 <span class=num>0%</span></div><progress class=progress value=0 max=100></progress></div><div class=toc-list><ol class=toc><li class="toc-item toc-level-2"><a class=toc-link href=#Background><span class=toc-text>Background</span></a></li><li class="toc-item toc-level-2"><a class=toc-link href=#TRANSFORMER-M><span class=toc-text>TRANSFORMER-M</span></a><ol class=toc-child><li class="toc-item toc-level-3"><a class=toc-link href=#Transformer-layer><span class=toc-text>Transformer layer</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Encoding-pair-wise-relations-in-E><span class=toc-text>Encoding pair-wise relations in E</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Encoding-pair-wise-relations-in-R><span class=toc-text>Encoding pair-wise relations in R</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Encoding-atom-wise-structural-information-in-E><span class=toc-text>Encoding atom-wise structural information in E</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Encoding-atom-wise-structural-information-in-R><span class=toc-text>Encoding atom-wise structural information in R</span></a></li></ol></li><li class="toc-item toc-level-2"><a class=toc-link href=#EXPERIMENTS><span class=toc-text>EXPERIMENTS</span></a><ol class=toc-child><li class="toc-item toc-level-3"><a class=toc-link href=#PCQM4MV2-PERFORMANCE-2D><span class=toc-text>PCQM4MV2 PERFORMANCE (2D)</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#PDBBIND-PERFORMANCE-2D-amp-3D><span class=toc-text>PDBBIND PERFORMANCE (2D &amp; 3D)</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#QM9-PERFORMANCE-3D><span class=toc-text>QM9 PERFORMANCE (3D)</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#ABLATION-STUDY><span class=toc-text>ABLATION STUDY</span></a><ol class=toc-child><li class="toc-item toc-level-4"><a class=toc-link href=#Impact-of-the-pre-training-tasks><span class=toc-text>Impact of the pre-training tasks</span></a></li><li class="toc-item toc-level-4"><a class=toc-link href=#Impact-of-mode-distribution><span class=toc-text>Impact of mode distribution</span></a></li></ol></li></ol></li></ol></div></div></div></main><section id=rightside><div class=rightside-btn><a harf=javascript: id=darkmode title=深色/浅色><i class="fas fa-moon"></i></a></div><div class=rightside-item><a harf=javascript: id=darkmode title=深色/浅色><i class="fas fa-moon"></i> </a><a href=javascript: id=settings title=设置><i class="fas fa-cog fa-spin"></i> </a><a id=open-toc title=目录><i class="fas fa-list-ul"></i> </a><a href=javascript:(0) id=darkmode title="深色/浅色 "><i class="fas fa-moon"></i> </a><a href=#直达评论 title=直达评论><i class="fas fa-comments"></i> </a><a href=# title=回到顶部><i class="fas fa-arrow-up"></i></a></div></section><footer class=footer id=footer><div class=copyright>&copy; 2022 - 2023 <i class="fas fa-fan"></i> magmell</div><div class=framework-info><span>框架 </span><a href=https://hexo.io target=_blank>Hexo</a> <span class=footer-separator>|</span> <span>主题 </span><a href=https://github.com/lete114/hexo-theme-MengD target=_blank>MengD.(萌典)</a></div></footer></div><div id=mask onclick='"use strict";closeAll();'></div><div id=local-search><div id=local-search-title>本地搜索</div><input id=local-search-input autocomplete=off placeholder=搜索文章 type=text><hr><div id=local-search-result></div><span class=search-close-button><i class="fas fa-times"></i></span></div><script src=/js/search.js>"use strict";</script><div class=script><script src=/js/main.js>"use strict";</script><script src=/js/lazyload.js>"use strict";</script><script src=/MathJax.js>"use strict";</script><script>"use strict";function LoadPjax(){new Pjax({selectors:["head title",'head meta[name="keywords"]','head meta[name="description"]',"main","#rightside"],cache:!0,cacheBust:!1})}getScript("https://cdn.jsdelivr.net/npm/pjax/pjax.min.js",LoadPjax);var timer=null;function ProgressStart(){var t=10,e=document.createElement("div");e.className="pjax-progress",document.body.prepend(e);clearInterval(timer),timer=setInterval(function(){var e=parseInt(7*Math.random());t+=e+3,document.getElementsByClassName("pjax-progress")[0].style.width=t+"%",95<t&&(t=95)},500)}function ProgressFinish(){clearInterval(timer);var e=document.getElementsByClassName("pjax-progress");e[0].style.width="100%",timer=setTimeout(function(){e[0].parentNode.removeChild(e[0])},700)}document.addEventListener("pjax:send",function(){ProgressStart()}),document.addEventListener("pjax:complete",function(){ProgressFinish(),exeAllFn(),ImgLazyLoad("body img[data-img]","data-img"),document.querySelectorAll("script[data-pjax]").forEach(function(e){var t=document.createElement("script"),a=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(function(e){return t.setAttribute(e.name,e.value)}),t.appendChild(document.createTextNode(a)),e.parentNode.replaceChild(t,e)})}),document.addEventListener("pjax:error",function(e){404===e.request.status&&pjax.loadUrl("/404.html")});</script></div></body><body><script src=/live2d/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05>"use strict";</script><script>"use strict";L2Dwidget.init({pluginRootPath:"live2d/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2d/assets/chtholly.model.json"},display:{position:"left",width:200,height:400,hOffset:-30,vOffset:-55},mobile:{show:!0,scale:.4},react:{opacity:.7},log:!1});</script></body></html>