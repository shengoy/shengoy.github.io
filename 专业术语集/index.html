<!DOCTYPE html><html><head><meta charset=UTF-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0"><title>专业术语集 | magmell</title><meta name=author content=magmell><meta name=copyright content=magmell><link rel="shortcut icon" href=http://magmell.site/img/My.jpg><meta name=keywords content=整理><meta name=description content="​    整理一些经常遇到的一些专业名词，属于个人主观理解，仅提供感性认识，详细信息请查阅专业文献！持续更新……   编辑距离衡量两个字符串的差异程度。假设有str1和str2两个字符串，编辑距离指的是str1经过多少次变化可以得到str2。一次变化指的是对单个字符的插入、删除、替换操作。这个经常出现在算法题中，可有以下递归式解出。  i，j分别时str1和str2的下标，lev（i，j）表示字"><meta property=og:type content=article><meta property=og:title content=专业术语集><meta property=og:url content=http://magmell.site/%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E9%9B%86/ ><meta property=og:site_name content=magmell><meta property=og:description content="​    整理一些经常遇到的一些专业名词，属于个人主观理解，仅提供感性认识，详细信息请查阅专业文献！持续更新……   编辑距离衡量两个字符串的差异程度。假设有str1和str2两个字符串，编辑距离指的是str1经过多少次变化可以得到str2。一次变化指的是对单个字符的插入、删除、替换操作。这个经常出现在算法题中，可有以下递归式解出。  i，j分别时str1和str2的下标，lev（i，j）表示字"><meta property=og:locale><meta property=og:image content=http://magmell.site/img/My.jpg><meta property=article:published_time content=2022-04-03T09:45:09.000Z><meta property=article:modified_time content=2022-08-02T01:59:18.000Z><meta property=article:author content=magmell><meta property=article:tag content=整理><meta name=twitter:card content=summary><meta name=twitter:image content=http://magmell.site/img/My.jpg><meta http-equiv=Cache-Control content=no-siteapp><link href=//cdn.jsdelivr.net rel=preconnect><link href=http://magmell.site rel=prefetch><link href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css rel=stylesheet><link href=/css/main.css rel=stylesheet><script src=/js/utlis.js>"use strict";</script><meta name=generator content="Hexo 5.4.1"></head><body><script>"use strict";var $config={tocStyle:"visible",CDN:{fancyboxJs:"https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js",fancyboxCss:"https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css"},searchFile:"/search.xml",codeBlockExpand:{enable:!0,height:400,scrollTop:200}};</script><script>"use strict";var _hmt=_hmt||[];!function(){var t=document.createElement("script"),e=(t.src="https://hm.baidu.com/hm.js?b8312b70c827f3279376517c136f52f5",document.getElementsByTagName("script")[0]);e.parentNode.insertBefore(t,e)}();</script><script>"use strict";var script=document.createElement("script");function gtag(){dataLayer.push(arguments)}script.src="https://www.googletagmanager.com/gtag/js?id=G-DHWF0JVX7P",script.async=!0,document.head.appendChild(script),window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DHWF0JVX7P");</script><script>"use strict";document.addEventListener("pjax:complete",function(){"undefined"!=typeof _hmt&&"function"==typeof _hmt.push&&_hmt.push(["_trackPageview",window.location.pathname]),"function"==typeof ga&&ga("send","pageview",window.location.href),"function"==typeof gtag&&gtag("event","pageview",{page_location:window.location.href})});</script><div id=body-wrap><nav id=nav-wrap><div class=navbar><div class=bar><a href=/ class=title>magmell</a> <i class="fas fa-search search-btn"></i><ul class=menu><li><a href=/ >首页</a></li><li><a class=menu-child-hover href=javascript:void(0);>找文章</a><ul class=menu-child><li><a href=/tags>标签</a></li><li><a href=/categories>分类</a></li><li><a href=/archives>归档</a></li></ul></li><li><a href=/about>关于我</a></li></ul><i class="fas fa-bars open-nav"></i></div></div><div id=mobile-nav><ul><li><a href=/ >首页</a></li><li><a href=/tags>标签</a></li><li><a href=/categories>分类</a></li><li><a href=/archives>归档</a></li><li><a href=/about>关于我</a></li></ul></div></nav><main id=main><article id=post><div class=post-info><div class=post-title><h1>专业术语集</h1></div><div class=post-meta><div class=post-date><i class="far fa-calendar-alt fa-fw post-meta-icon"></i> <span class=post-meta-label>发表于 2022-04-03 | </span><i class="fas fa-history fa-fw post-meta-icon"></i> <span class=post-meta-label>更新于 2023-07-24</span></div><div class=post-meta-wordcount><i class="far fa-file-word fa-fw post-meta-icon"></i> <span class=post-meta-label>总字数:</span> <span class=word-count>1.8k | </span><i class="far fa-clock fa-fw post-meta-icon"></i> <span class=post-meta-label>阅读时长:</span> <span>6分钟</span> | <i class="far fa-eye fa-fw post-meta-icon"></i> <span id=/%E4%B8%93%E4%B8%9A%E6%9C%AF%E8%AF%AD%E9%9B%86/ class=leancloud_visitors data-flag-title=专业术语集><span class=post-meta-label>阅读量:</span> <span class=leancloud-visitors-count>0</span></span></div></div></div><div class=post-content><blockquote><p>​ <u>整理一些经常遇到的一些专业名词，属于个人主观理解，仅提供感性认识，详细信息请查阅专业文献！持续更新……</u></p></blockquote><hr><h3 id=编辑距离><a href=#编辑距离 class=headerlink title=编辑距离></a><center>编辑距离</center></h3><p>衡量两个字符串的差异程度。假设有str1和str2两个字符串，编辑距离指的是str1经过多少次变化可以得到str2。一次变化指的是对单个字符的插入、删除、替换操作。这个经常出现在算法题中，可有以下递归式解出。</p><p><img src=https://tvax1.sinaimg.cn/large/006XaTquly1h0wt3w2faoj31kw0bj417.jpg alt=image></p><p>i，j分别时str1和str2的下标，<strong>lev（i，j）</strong>表示字符串str1前i个字符（从1开始算包括i）和str2前j个字符的编辑距离。</p><h3 id=Inductive-bias><a href=#Inductive-bias class=headerlink title="Inductive bias"></a><center>Inductive bias</center></h3><p>翻译叫归纳偏置，特别不好理解，其实应该可以说是先验知识。比如说有人跟你提到一种你从没见过的狗狗，但是根据以往的知识你可以想得到这狗狗应该有四条腿，一条尾巴，<del>以及喜欢吃翔……</del> 你对未知事物的一个提前认知就可以称作先验知识。</p><h3 id=Self-supervised-Learning><a href=#Self-supervised-Learning class=headerlink title="Self-supervised Learning"></a><center>Self-supervised Learning</center></h3><p>自监督学习，顾名思义就是自己监督自己。人工标注的数据是很昂贵的，所以一个自然的想法就是能不能让从数据本身产生“标签”，之所以打个引号，是因为这个“标签”和传统的标注这张图是猫还是狗的标签不太一样。比如在自然语言处理中经常用一句话的前几个词去预测下一个词，具体来说，对于“我想吃苹果”这句话，现在知道了“我 想 吃”三个词，目标是预测下一个词“苹果”，而“苹果”这个词就是这个样本的“标签”。</p><h3 id=Auto-encoder-（AE）and-Variational-auto-encoder（VAE）><a href=#Auto-encoder-（AE）and-Variational-auto-encoder（VAE） class=headerlink title="Auto-encoder （AE）and Variational auto-encoder（VAE）"></a><center>Auto-encoder （AE）and Variational auto-encoder（VAE）</center></h3><p>两者主要利用的是自监督的思想，把一个东西，比如图片，扔进编码器，得到一个向量，再把这个向量扔到解码器里面，我们希望根据这个向量的信息经过解码器能得到原来那个东西。下图是自动编码器的示意图：</p><p><img src=https://tvax2.sinaimg.cn/large/006XaTquly1h0wtyrmaevj306106gweg.jpg alt=AE></p><p>你可能会奇怪，费老大劲就为了得到一个和原来一样的东西（其实也不能说是一样，一般来说肯定会有损失）为了啥？有啥用？要是说得到的这张图确实没啥用，但是中间那个向量就有用了。比如说上面图片中，蒙娜丽萨的图片经过编码器得到一个向量，而这个向量经过解码器可以还原蒙娜丽萨的图片。这就意味着这个向量包含了这张图片的大部分信息，也就是说，可以用这个向量来表示这张图片，可以看成是数据的降维或者特征抽取之类的巴拉巴拉……</p><p>AE的方法只能得到一个图片的向量表示，我们也可以用这个向量来还原图像，但是我们如果想随便用一个向量来经过编码器生成一张新的“合理”的图片这是办不到的。因为能还原图像的向量是经过编码器得到的“合理”向量，而随便拿一个向量极大可能是不“合理”的。而变分自动编码器就是为了解决这样一个问题，如下图：</p><p><img src=https://tva1.sinaimg.cn/large/006XaTquly1h0wv5ac6apj3095049a9z.jpg></p><p>在自动编码器（AE）的基础上对生成的向量添加了一些约束，让生成的变量服从高斯分布（也就是正态分布）。这样我们从这个高斯分布中随机采样得到向量基本上是合理的变量，通过这种方式我们就可以“生成”图片了，而不仅仅是存储图片。</p><p>这里面数学原理还是蛮有趣的，我最近也在了解。</p><h3 id=Pre-train-and-Fine-tune><a href=#Pre-train-and-Fine-tune class=headerlink title="Pre-train and Fine-tune"></a><center>Pre-train and Fine-tune</center></h3><p><strong>Pre-train</strong> 已经训练好可以直接拿来用的模型参数，而不需要从头开始训练（train from scratch）</p><p><strong>Fine-tune</strong> 在pre-train参数基础上根据下游任务进行调整，通常截断最后一层（softmax）</p><p>一个直观的理解是一座高山，比如五岳，上面有很多景点，是我们要去的地方。我们没法直达景点，又不想从山底下开始爬，所以我们可以坐缆车到山腰，再自己走到想去的景点。坐缆车到山腰等于就是利用了pre-train，省了不少事，各个想去的景点就是下游任务，我们根据自己的目的地从山腰调整我们的路线就是fine-tune。</p><p>pre-train在自然语言处理（NLP）中应用很广泛，比如pre-train word embedding 比如 <strong>word2vec</strong>，通常作为初始化模型的第一层，属于比较浅层的方法，用表达能力换效率。缺点是每个词对应一个固定的vector，但是很多情况下一个词是有很多含义的，即一词多义，与它所处的语境有关。pre-train language model 比如 ELMo能学习到更深层次的特征，一定情况下可以缓解这个问题。</p><p>为什么预训练模型泛化性能好？可能原因：由于训练数据足够大，可以归纳出一组假设空间，而这组假设空间包含了大部分任务（这也要求预训练任务应该是比较通用的）。fine-tune就是从中选出一个最优的假设。</p><h3 id=Domain-adaptation-（DA）><a href=#Domain-adaptation-（DA） class=headerlink title="Domain adaptation （DA）"></a><center>Domain adaptation （DA）</center></h3><p>为了实现在target domain中逼近在source domain中的表现效果而引入的手段，使得模型更普适。是迁移学习中一种代表性方法。</p><h3 id=Dropout><a href=#Dropout class=headerlink title=Dropout></a><center>Dropout</center></h3><p>训练网络的一个trick，让神经元以一定概率停止工作。</p><p>背景：模型参数过多，训练样本太少，容易过拟合</p><p>可能原因：</p><ul><li>取平均作用。因为dropout掉不同的神经元得到不同的网络，而最后的结果可以看作不同网络取平均，不同网络的过拟合可能会相互抵消。</li><li>减少对特定特征的依赖，提高泛化性能。比如要训练一个识别树叶的网络，训练集中的叶子边缘呈锯齿状，但是不是所有的树叶都有锯齿状的边缘，通过dropout可能会破坏掉这个边缘特征。</li></ul><p>dropout可以有效缓解过拟合，在一定程度上达到正则化的效果。</p><h3 id=end-to-end><a href=#end-to-end class=headerlink title=end-to-end></a><center>end-to-end</center></h3><p>我查到有两种解释：</p><ol><li>输入原始数据，输出最后结果</li><li>joint learning 将多个阶段&#x2F;多步堆在一起优化（因为无法保证各阶段最优合起来还是最优），即将pipline变为单个网络</li></ol><p>这两者有一个共同点，就是把模型当成一个黑箱子，我们希望扔进出数据就能得到相应结果</p><h3 id=non-trival><a href=#non-trival class=headerlink title=non-trival></a><center>non-trival</center></h3><p>是指具有一定复杂度，需要一定脑力活动加工才能得到的（结果、结论、实现……）</p><h3 id=Sparse-matrix><a href=#Sparse-matrix class=headerlink title="Sparse matrix"></a><center>Sparse matrix</center></h3><p>稀疏矩阵，大部分元素为0的矩阵。</p><p>稀疏矩阵的存储：</p><ul><li>支持有效修改<ul><li>**Dictionary of keys (DOK)**：用字典形式存储，key是行和列的索引，value是该位置的值，</li><li>**List of lists (LIL)**：每一行存储一个列表</li></ul></li></ul></div><div class=post-copyright><div class=post-copyright-icon></div><div class=post-copyright-author><span class=post-copyright-meta>文章作者: </span><span class=post-copyright-info><a href=mailto:shengouy@stu.xmu.edu.cn>shoy</a></span></div><div class=post-copyright-type><span class=post-copyright-meta>文章链接: </span><span class=post-copyright-info><a href=http://magmell.site/专业术语集/ >http://magmell.site/专业术语集/</a></span></div><div class=post-copyright-notice><span class=post-copyright-meta>版权声明: </span><span class=post-copyright-info>本博客所有文章除特别声明外，均采用 <a href=https://creativecommons.org/licenses/by-nc-sa/4.0/ target=_blank>CC BY-NC-SA 4.0 </a>许可协议。转载请注明来自 <a href=http://magmell.site/专业术语集/ target=_blank>blog from magmell</a> ！</span></div></div><div class=post-tag><span><i class="fa fa-tag"></i> <a href=tags/整理/ ><span>整理</span></a></span></div><div class=pagination-post><a href=/Some-notes-about-survey-research/ ><div class=prev-title><i class="fas fa-chevron-left"></i>Some notes about survey &amp; research</div><div class=prev-desc>一些碎片化的感想，大部分内容来自台大资管系主任庄裕泽教授写给学生们的信件。（偏理工科方向 做一个survey，要有一个主线框架，比如按照方法、数据、应用场景巴拉巴拉之类的分个类。可以有大量...</div></a><a href=/%E6%BE%A1%E5%A0%82%E9%9B%86/ ><div class=next-title>澡堂集<i class="fas fa-chevron-right"></i></div><div class=next-desc>南方人刚开始来帝都上学面对公共澡堂还是有点不知所措的（才没有，老夫高中就是大澡堂子hhh），慢慢地也就适应了，而且相对于独立卫浴大澡堂也别有一番趣味。本篇记录一些澡堂的发生的小故事（内容纯属瞎编...</div></a></div><div class=comment-head id=直达评论><hr><div class=comment-headline><i class="fas fa-comments fa-fw"></i> <span>评论</span></div><div id=vcomments></div><script>"use strict";function LoadValine(){getScript("https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js",function(){var e={el:"#vcomments",path:window.location.pathname,appId:"uYPQylOCyVdXUjwoeA1Qlo9r-gzGzoHsz",appKey:"fhFK17SLElipGtq3bmkD9AGi",master:"",friends:"",placeholder:"快来评论吧!!",avatar:"",meta:"nick,mail,link".split(","),pageSize:5,lang:"zh-CN",emojiCDN:"",emojiMaps:null,enableQQ:!0,visitor:!0};new Valine(e)})}LoadValine();</script></div></article><div id=toc-wrap><div id=toc><div class=toc-title><div>目录 <span class=num>0%</span></div><progress class=progress value=0 max=100></progress></div><div class=toc-list><ol class=toc><li class="toc-item toc-level-3"><a class=toc-link href=#%E7%BC%96%E8%BE%91%E8%B7%9D%E7%A6%BB><span class=toc-text>编辑距离</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Inductive-bias><span class=toc-text>Inductive bias</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Self-supervised-Learning><span class=toc-text>Self-supervised Learning</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Auto-encoder-%EF%BC%88AE%EF%BC%89and-Variational-auto-encoder%EF%BC%88VAE%EF%BC%89><span class=toc-text>Auto-encoder （AE）and Variational auto-encoder（VAE）</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Pre-train-and-Fine-tune><span class=toc-text>Pre-train and Fine-tune</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Domain-adaptation-%EF%BC%88DA%EF%BC%89><span class=toc-text>Domain adaptation （DA）</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Dropout><span class=toc-text>Dropout</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#end-to-end><span class=toc-text>end-to-end</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#non-trival><span class=toc-text>non-trival</span></a></li><li class="toc-item toc-level-3"><a class=toc-link href=#Sparse-matrix><span class=toc-text>Sparse matrix</span></a></li></ol></div></div></div></main><section id=rightside><div class=rightside-btn><a harf=javascript: id=darkmode title=深色/浅色><i class="fas fa-moon"></i></a></div><div class=rightside-item><a harf=javascript: id=darkmode title=深色/浅色><i class="fas fa-moon"></i> </a><a href=javascript: id=settings title=设置><i class="fas fa-cog fa-spin"></i> </a><a id=open-toc title=目录><i class="fas fa-list-ul"></i> </a><a href=javascript:(0) id=darkmode title="深色/浅色 "><i class="fas fa-moon"></i> </a><a href=#直达评论 title=直达评论><i class="fas fa-comments"></i> </a><a href=# title=回到顶部><i class="fas fa-arrow-up"></i></a></div></section><footer class=footer id=footer><div class=copyright>&copy; 2022 - 2023 <i class="fas fa-fan"></i> magmell</div><div class=framework-info><span>框架 </span><a href=https://hexo.io target=_blank>Hexo</a> <span class=footer-separator>|</span> <span>主题 </span><a href=https://github.com/lete114/hexo-theme-MengD target=_blank>MengD.(萌典)</a></div></footer></div><div id=mask onclick='"use strict";closeAll();'></div><div id=local-search><div id=local-search-title>本地搜索</div><input id=local-search-input autocomplete=off placeholder=搜索文章 type=text><hr><div id=local-search-result></div><span class=search-close-button><i class="fas fa-times"></i></span></div><script src=/js/search.js>"use strict";</script><div class=script><script src=/js/main.js>"use strict";</script><script src=/js/lazyload.js>"use strict";</script><script src=/MathJax.js>"use strict";</script><script>"use strict";function LoadPjax(){new Pjax({selectors:["head title",'head meta[name="keywords"]','head meta[name="description"]',"main","#rightside"],cache:!0,cacheBust:!1})}getScript("https://cdn.jsdelivr.net/npm/pjax/pjax.min.js",LoadPjax);var timer=null;function ProgressStart(){var t=10,e=document.createElement("div");e.className="pjax-progress",document.body.prepend(e);clearInterval(timer),timer=setInterval(function(){var e=parseInt(7*Math.random());t+=e+3,document.getElementsByClassName("pjax-progress")[0].style.width=t+"%",95<t&&(t=95)},500)}function ProgressFinish(){clearInterval(timer);var e=document.getElementsByClassName("pjax-progress");e[0].style.width="100%",timer=setTimeout(function(){e[0].parentNode.removeChild(e[0])},700)}document.addEventListener("pjax:send",function(){ProgressStart()}),document.addEventListener("pjax:complete",function(){ProgressFinish(),exeAllFn(),ImgLazyLoad("body img[data-img]","data-img"),document.querySelectorAll("script[data-pjax]").forEach(function(e){var t=document.createElement("script"),a=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(function(e){return t.setAttribute(e.name,e.value)}),t.appendChild(document.createTextNode(a)),e.parentNode.replaceChild(t,e)})}),document.addEventListener("pjax:error",function(e){404===e.request.status&&pjax.loadUrl("/404.html")});</script></div></body><body><script src=/live2d/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05>"use strict";</script><script>"use strict";L2Dwidget.init({pluginRootPath:"live2d/",pluginJsPath:"lib/",pluginModelPath:"assets/",tagMode:!1,debug:!1,model:{jsonPath:"/live2d/assets/chtholly.model.json"},display:{position:"left",width:200,height:400,hOffset:-30,vOffset:-55},mobile:{show:!0,scale:.4},react:{opacity:.7},log:!1});</script></body></html>